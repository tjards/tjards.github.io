<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Featured | research notes</title>
<meta name=keywords content><meta name=description content="Personal Website"><meta name=author content="tjards"><link rel=canonical href=http://localhost:1313/tags/featured/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/img/favicon-quarrg.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/img/favicon-quarrg.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/img/favicon-quarrg.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=http://localhost:1313/tags/featured/index.xml><link rel=alternate hreflang=en href=http://localhost:1313/tags/featured/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=/css/style.css><meta property="og:url" content="http://localhost:1313/tags/featured/"><meta property="og:site_name" content="research notes"><meta property="og:title" content="Featured"><meta property="og:description" content="Personal Website"><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/style.css></head><body class=list id=top><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title="about me"><span>about me</span></a></li><li><a href=http://localhost:1313/archives/ title=posts><span>posts</span></a></li><li><a href=http://localhost:1313/projects/ title=projects><span>projects</span></a></li><li><a href=http://localhost:1313/categories/personal/ title=annex><span>annex</span></a></li><li><a href="https://scholar.google.com/citations?user=RGlv4ZUAAAAJ&amp;hl=en" title=publications><span>publications</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/tags/>tags</a></div><h1>Featured</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>The Future of AI is in Lower Dimensions</h2></header><div class=entry-content><p>In a recent interview with Lex Fridman entitled “Dark Matter of Intelligence and Self-Supervised Learning,” outspoken AI pioneer Yann Lecun suggested the next leap in Artificial Intelligence (AI) will come from learning in lower-dimensional latent spaces.
“You don’t predict pixels, you predict an abstract representation of pixels.” - Yann Lecun
What does he mean and how is it relevant to the future of AI?
Let’s back up and consider the context in which this statement was made. Yann was discussing the limitations of current AI systems, particularly those based on deep neural networks. In a previous article, we touched on one such example — Large Language Models (LLMs). LLMs have demonstrated impressive performance across an array of language-related tasks. So popular, a recent AWS study found a “shocking amount of the web” is already LLM-generated. This is problematic, as LLMs trained on this kind of synthetic content break down and lose their ability to generalize. A recent Nature article described this “model collapse” phenomenon in detail.
...</p></div><footer class=entry-footer><span title='2025-09-29 17:18:41 -0400 EDT'>29 Sep 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;tjards</footer><a class=entry-link aria-label="post link to The Future of AI is in Lower Dimensions" href=http://localhost:1313/posts/2025/hypospace_learn/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>The Machines Built The Matrix to Avoid Model Collapse</h2></header><div class=entry-content><p>A new theory for why the Machines kept humans alive in The Matrix —inspired by recent discoveries in scaling large language models.
One measure of a film’s quality is the diversity of fan theories it inspires. When a story has the right blend of depth, ambiguity, and cultural timing, the entertainment value extends past the credits—it compels audiences to dissect and reinterpret decades later. The Matrix is a great example of this: 25 years on, people are still following the white rabbit down Reddit threads. A quick internet search reveals a myriad of fan theories about the true nature of its characters and storylines. One even claims John Wick is actually a sequel to The Matrix.
...</p></div><footer class=entry-footer><span title='2025-06-08 13:42:09 -0400 EDT'>8 Jun 2025</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;tjards</footer><a class=entry-link aria-label="post link to The Machines Built The Matrix to Avoid Model Collapse" href=http://localhost:1313/posts/2025/matrix_entropy/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>New publication in Automatica!</h2></header><div class=entry-content><p>Emergent homeomorphic curves in swarms This work introduces the concept of geometric embeddings, which permit the application of linear control policies to produce globally-stable emergent curves in swarms of unmanned aerial vehicles. The vehicles make decisions based only on local observations, without knowing their role in the larger group. Below is an animation of the technique being used to produce a lemniscatic arc.
Article available here.
Code available here.
...</p></div><footer class=entry-footer><span title='2025-02-28 21:14:56 -0400 -0400'>28 Feb 2025</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;tjards</footer><a class=entry-link aria-label="post link to New publication in Automatica!" href=http://localhost:1313/posts/2025/emergent_homeomorphic/></a></article></main><footer class=site-footer><div class=footer-links><a href=/tags>tags</a>
<a href=/categories>categories</a></div><div class=container><p>Disclaimer: Views expressed here are my own and do not represent the opinions of my associates or my employer.</p><p>&copy; tjards 2025 |
Licenced under <a href=https://creativecommons.org/licenses/by/4.0/ target=_blank>Creative Commons</a>.</p><p>Built with <a href=https://gohugo.io/ target=_blank>Hugo</a>
using <a href=https://github.com/adityatelange/hugo-PaperMod target=_blank>PaperMod</a> theme.</p></footer></body></html>