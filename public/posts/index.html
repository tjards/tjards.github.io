<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Posts | research notes</title>
<meta name=keywords content><meta name=description content="Posts - research notes"><meta name=author content="tjards"><link rel=canonical href=http://localhost:1313/posts/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/img/cloud-l.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/img/cloud-l.jpeg><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/img/cloud-l.jpeg><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=http://localhost:1313/posts/index.xml><link rel=alternate hreflang=en href=http://localhost:1313/posts/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=/css/style.css><meta property="og:url" content="http://localhost:1313/posts/"><meta property="og:site_name" content="research notes"><meta property="og:title" content="Posts"><meta property="og:description" content="Personal Website"><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"}]}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/style.css></head><body class=list id=top><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title="about me"><span>about me</span></a></li><li><a href=http://localhost:1313/archives/ title=posts><span>posts</span></a></li><li><a href=http://localhost:1313/projects/ title=projects><span>projects</span></a></li><li><a href=http://localhost:1313/categories/personal/ title=annex><span>annex</span></a></li><li><a href="https://scholar.google.com/citations?user=RGlv4ZUAAAAJ&amp;hl=en" title=publications><span>publications</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a></div><h1>Posts</h1></header><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>The Future of AI is in Lower Dimensions</h2></header><div class=entry-content><p>In a recent interview with Lex Fridman entitled “Dark Matter of Intelligence and Self-Supervised Learning,” outspoken AI pioneer Yann Lecun suggested the next leap in Artificial Intelligence (AI) will come from learning in lower-dimensional latent spaces.
“You don’t predict pixels, you predict an abstract representation of pixels.” - Yann Lecun
What does he mean and how is it relevant to the future of AI?
Let’s back up and consider the context in which this statement was made. Yann was discussing the limitations of current AI systems, particularly those based on deep neural networks. In a previous article, we touched on one such example — Large Language Models (LLMs). LLMs have demonstrated impressive performance across an array of language-related tasks. So popular, a recent AWS study found a “shocking amount of the web” is already LLM-generated. This is problematic, as LLMs trained on this kind of synthetic content break down and lose their ability to generalize. A recent Nature article described this “model collapse” phenomenon in detail.
...</p></div><footer class=entry-footer><span title='2025-09-29 17:18:41 -0400 EDT'>29 Sep 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;tjards</footer><a class=entry-link aria-label="post link to The Future of AI is in Lower Dimensions" href=http://localhost:1313/posts/2025/hypospace_learn/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>The Machines Built The Matrix to Avoid Model Collapse</h2></header><div class=entry-content><p>A new theory for why the Machines kept humans alive in The Matrix —inspired by recent discoveries in scaling large language models.
One measure of a film’s quality is the diversity of fan theories it inspires. When a story has the right blend of depth, ambiguity, and cultural timing, the entertainment value extends past the credits—it compels audiences to dissect and reinterpret decades later. The Matrix is a great example of this: 25 years on, people are still following the white rabbit down Reddit threads. A quick internet search reveals a myriad of fan theories about the true nature of its characters and storylines. One even claims John Wick is actually a sequel to The Matrix.
...</p></div><footer class=entry-footer><span title='2025-06-08 13:42:09 -0400 EDT'>8 Jun 2025</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;tjards</footer><a class=entry-link aria-label="post link to The Machines Built The Matrix to Avoid Model Collapse" href=http://localhost:1313/posts/2025/matrix_entropy/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Go North
<span class=entry-hint title=Draft><svg height="20" viewBox="0 -960 960 960" fill="currentcolor"><path d="M160-410v-60h3e2v60H160zm0-165v-60h470v60H160zm0-165v-60h470v60H160zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22-4.5 22.5T862.09-380L643-160H520zm3e2-263-37-37 37 37zM580-220h38l121-122-18-19-19-18-122 121v38zm141-141-19-18 37 37-18-19z"/></svg></span></h2></header><div class=entry-content><p>Man digs hole. Finds precious object. Takes it home. Wipes it clean. Genie appears. Genie says, “I will grant you one wish.” Man says, “I expected three.” Genie says, “Only one - and you will have to work for it.” Man says, “Then I want to live forever.” Compass appears in man’s hand. Folded paper in the other. ...</p></div><footer class=entry-footer><span title='2025-05-31 10:32:54 -0400 EDT'>31 May 2025</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;tjards</footer><a class=entry-link aria-label="post link to Go North" href=http://localhost:1313/posts/2025/gonorth/></a></article><footer class=page-footer><nav class=pagination><a class=next href=http://localhost:1313/posts/page/2/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=site-footer><div class=footer-links><a href=/tags>tags</a>
<a href=/categories>categories</a></div><div class=container><p>Disclaimer: Views expressed here are my own and do not represent the opinions of my associates or my employer.</p><p>&copy; tjards 2025 |
Licenced under <a href=https://creativecommons.org/licenses/by/4.0/ target=_blank>Creative Commons</a>.</p><p>Built with <a href=https://gohugo.io/ target=_blank>Hugo</a>
using <a href=https://github.com/adityatelange/hugo-PaperMod target=_blank>PaperMod</a> theme.</p></footer></body></html>