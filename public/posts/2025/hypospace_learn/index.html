<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>The Future of AI is in Lower Dimensions | research notes</title>
<meta name=keywords content="featured,ai"><meta name=description content="In a recent interview with Lex Fridman entitled &ldquo;Dark Matter of Intelligence and Self-Supervised Learning,&rdquo; outspoken AI pioneer Yann Lecun suggested the next leap in Artificial Intelligence (AI) will come from learning in lower-dimensional latent spaces.

&ldquo;You don&rsquo;t predict pixels, you predict an abstract representation of pixels.&rdquo; - Yann Lecun
What does he mean and how is it relevant to the future of AI?
Let&rsquo;s back up and consider the context in which this statement was made. Yann was discussing the limitations of current AI systems, particularly those based on deep neural networks. In a previous article, we touched on one such example — Large Language Models (LLMs). LLMs have demonstrated impressive performance across an array of language-related tasks. So popular, a recent AWS study found a &ldquo;shocking amount of the web&rdquo; is already LLM-generated. This is problematic, as LLMs trained on this kind of synthetic content break down and lose their ability to generalize. A recent Nature article described this &ldquo;model collapse&rdquo; phenomenon in detail."><meta name=author content="tjards"><link rel=canonical href=http://localhost:1313/posts/2025/hypospace_learn/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/img/favicon-quarrg.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/img/favicon-quarrg.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/img/favicon-quarrg.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/2025/hypospace_learn/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=/css/style.css><meta property="og:url" content="http://localhost:1313/posts/2025/hypospace_learn/"><meta property="og:site_name" content="research notes"><meta property="og:title" content="The Future of AI is in Lower Dimensions"><meta property="og:description" content="In a recent interview with Lex Fridman entitled “Dark Matter of Intelligence and Self-Supervised Learning,” outspoken AI pioneer Yann Lecun suggested the next leap in Artificial Intelligence (AI) will come from learning in lower-dimensional latent spaces.
“You don’t predict pixels, you predict an abstract representation of pixels.” - Yann Lecun
What does he mean and how is it relevant to the future of AI?
Let’s back up and consider the context in which this statement was made. Yann was discussing the limitations of current AI systems, particularly those based on deep neural networks. In a previous article, we touched on one such example — Large Language Models (LLMs). LLMs have demonstrated impressive performance across an array of language-related tasks. So popular, a recent AWS study found a “shocking amount of the web” is already LLM-generated. This is problematic, as LLMs trained on this kind of synthetic content break down and lose their ability to generalize. A recent Nature article described this “model collapse” phenomenon in detail."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-29T17:18:41-04:00"><meta property="article:modified_time" content="2025-09-29T17:18:41-04:00"><meta property="article:tag" content="Featured"><meta property="article:tag" content="Ai"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"The Future of AI is in Lower Dimensions","item":"http://localhost:1313/posts/2025/hypospace_learn/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"The Future of AI is in Lower Dimensions","name":"The Future of AI is in Lower Dimensions","description":"In a recent interview with Lex Fridman entitled \u0026ldquo;Dark Matter of Intelligence and Self-Supervised Learning,\u0026rdquo; outspoken AI pioneer Yann Lecun suggested the next leap in Artificial Intelligence (AI) will come from learning in lower-dimensional latent spaces.\n\u0026ldquo;You don\u0026rsquo;t predict pixels, you predict an abstract representation of pixels.\u0026rdquo; - Yann Lecun\nWhat does he mean and how is it relevant to the future of AI?\nLet\u0026rsquo;s back up and consider the context in which this statement was made. Yann was discussing the limitations of current AI systems, particularly those based on deep neural networks. In a previous article, we touched on one such example — Large Language Models (LLMs). LLMs have demonstrated impressive performance across an array of language-related tasks. So popular, a recent AWS study found a \u0026ldquo;shocking amount of the web\u0026rdquo; is already LLM-generated. This is problematic, as LLMs trained on this kind of synthetic content break down and lose their ability to generalize. A recent Nature article described this \u0026ldquo;model collapse\u0026rdquo; phenomenon in detail.\n","keywords":["featured","ai"],"articleBody":"In a recent interview with Lex Fridman entitled “Dark Matter of Intelligence and Self-Supervised Learning,” outspoken AI pioneer Yann Lecun suggested the next leap in Artificial Intelligence (AI) will come from learning in lower-dimensional latent spaces.\n“You don’t predict pixels, you predict an abstract representation of pixels.” - Yann Lecun\nWhat does he mean and how is it relevant to the future of AI?\nLet’s back up and consider the context in which this statement was made. Yann was discussing the limitations of current AI systems, particularly those based on deep neural networks. In a previous article, we touched on one such example — Large Language Models (LLMs). LLMs have demonstrated impressive performance across an array of language-related tasks. So popular, a recent AWS study found a “shocking amount of the web” is already LLM-generated. This is problematic, as LLMs trained on this kind of synthetic content break down and lose their ability to generalize. A recent Nature article described this “model collapse” phenomenon in detail.\nBut Yann wasn’t just talking about LLMs. He was making a broader point about how these systems represent the world. Specifically, he was talking about a reliance on high-dimensional data and the need to move towards learning in lower-dimensional “latent spaces.”\nThe problem with high dimensions Contemporary models typically process raw sensor data like pixels, point clouds, or discrete tokens of text. These representations are big, complex, and noisy; they contain a lot of irrelevant information that makes learning computationally expensive, inefficient, and brittle. Tiny changes in lighting, for example, can throw off an image processing model, even though this information isn’t really related to what the object is in the real world.\nIf you think about it, this is exactly not how humans learn. We don’t process every little detail in our environment. No, we pull out the important features and ignore the things that don’t matter. Consider the example of catching a baseball. As it falls towards you, you’re not paying attention to the color, texture, or even shape of the ball. You’re certainly not thinking about the equations of motion that govern its flight. You’re focused on a few key features — where it is relative to your glove (in 2D space) and how much bigger it’s getting (in 1D space). From this, you make predictions about its path and adjust the position of your glove. The rest is noise.\nWhile simple dimensionality reduction techniques (like, for example, Principal Component Analysis) reduce complexity, Yann’s point is different — the goal is not just compression, but the discovery of causal abstractions that explain why the world behaves the way it does.\nLearning latent spaces Incorporating latent spaces into learning is not a new idea. They are already foundational to how many generative models work. Some diffusion models, for example, run certain processes in compressed latent spaces. Generative Adversarial Networks (GANs) do something similar, mapping meaningful representations of data from compact latent spaces; these can then be manipulated to control the generation process (e.g., add a hat to a person’s head).\nExisting uses of latent spaces are largely static and domain-specific. For instance, a latent space trained to recognize cats does not help a model understand gravity or motion. Yann argues that truly intelligent systems need to learn latent representations of the physical world as well. These latent spaces capture the essential structure of the environment so the models can make accurate predictions when the little things change. He lays out a blueprint in “A Path Towards Autonomous Machine Intelligence”.\nWhy this matters for the future of AI Surveying the current landscape of AI, one might be forgiven for thinking the future is all about bigger models, more data, and faster hardware. If Yann is correct, the future of AI will not be written in pixels or tokens, but the invisible spaces beneath them — abstract structures that capture how the world works while looking nothing like it.\nWe will see new architectures and methodologies that prioritize learning these latent spaces. Models will become more flexible and robust as they learn to focus on the underlying structure rather than surface-level details. They will need less data, burn less compute, and start to feel more like us.\nOur work We are exploring these concepts in our work on swarm robotics. In “Flocks, Mobs, and Figure Eights: Swarming as a Lemniscatic Arc”, we stabilized a swarm of agents in 3D space by controlling them indirectly through a 2D embedding space. This approach was motivated by a desire to use well-established linear control policies in reduced dimensions while preserving all of the functional properties of the swarm. We later generalized this approach for a broader set of curves in “Emergent homeomorphic curves in swarms”, where we reconceptualized this embedding as a latent space that captures the essential structure of the swarm.\nAfter exploring some of the properties of the latent space using dimensionality reduction techniques like Uniform Manifold Approximation and Projection (UMAP), we are now working to apply reinforcement learning (RL) to control the orientation of the swarm by manipulating the latent space.\nAs a proof of concept, we applied Continuous Action Learning Automata (CALA) to learn the optimal orientation of the latent space and reported the initial findings here. Collective learning was coordinated by a randomly selected leader agent. Below are two figures demonstrating, first, the evolution of the probability distribution functions for the two orientation parameters (altitude and azimuth) over time, and second, the combined rewards for each parameter over time.\nFig. 1 - Probability distribution function of altitude (blue) and azimuth (orange) parameters. Fig. 2 - Combined rewards for altitude (blue) and azimuth (orange) parameters over time. We have since incorporated this approach into our more general Multi-Agent Simulator (m-a_s) and are working on the following problems:\nMoving away from a leader-follower approach and towards more decentralized coordination of learning outcomes. Investigating more complex tasks. Exploring more sophisticated learning techniques. Analyzing stability and convergence properties. ","wordCount":"992","inLanguage":"en","datePublished":"2025-09-29T17:18:41-04:00","dateModified":"2025-09-29T17:18:41-04:00","author":{"@type":"Person","name":"tjards"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/2025/hypospace_learn/"},"publisher":{"@type":"Organization","name":"research notes","logo":{"@type":"ImageObject","url":"http://localhost:1313/img/favicon-quarrg.ico"}}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/style.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title="about me"><span>about me</span></a></li><li><a href=http://localhost:1313/archives/ title=posts><span>posts</span></a></li><li><a href=http://localhost:1313/projects/ title=projects><span>projects</span></a></li><li><a href=http://localhost:1313/categories/personal/ title=annex><span>annex</span></a></li><li><a href="https://scholar.google.com/citations?user=RGlv4ZUAAAAJ&amp;hl=en" title=publications><span>publications</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">The Future of AI is in Lower Dimensions</h1><div class=post-meta><span title='2025-09-29 17:18:41 -0400 EDT'>29 Sep 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;tjards</div><div class=post-tools><a class=tool-badge href=/tools/python>Python</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#the-problem-with-high-dimensions aria-label="The problem with high dimensions">The problem with high dimensions</a></li><li><a href=#learning-latent-spaces aria-label="Learning latent spaces">Learning latent spaces</a></li><li><a href=#why-this-matters-for-the-future-of-ai aria-label="Why this matters for the future of AI">Why this matters for the future of AI</a></li><li><a href=#our-work aria-label="Our work">Our work</a></li></ul></div></details></div><div class=post-content><p>In a recent interview with Lex Fridman entitled &ldquo;Dark Matter of Intelligence and Self-Supervised Learning,&rdquo; outspoken AI pioneer <a href=http://yann.lecun.com/>Yann Lecun</a> suggested the next leap in Artificial Intelligence (AI) will come from learning in lower-dimensional latent spaces.</p><blockquote><p><em>&ldquo;You don&rsquo;t predict pixels, you predict an abstract representation of pixels.&rdquo;</em> - Yann Lecun</p></blockquote><p>What does he mean and how is it relevant to the future of AI?</p><p>Let&rsquo;s back up and consider the context in which this statement was made. Yann was discussing the limitations of current AI systems, particularly those based on deep neural networks. In a <a href=/posts/2025/matrix_entropy>previous article</a>, we touched on one such example — Large Language Models (LLMs). LLMs have demonstrated impressive performance across an array of language-related tasks. So popular, a recent <a href=https://arxiv.org/abs/2401.05749>AWS study</a> found a &ldquo;shocking amount of the web&rdquo; is already LLM-generated. This is problematic, as LLMs trained on this kind of synthetic content break down and lose their ability to generalize. A recent <a href=https://www.nature.com/articles/s41586-024-07566-y>Nature article</a> described this &ldquo;model collapse&rdquo; phenomenon in detail.</p><p>But Yann wasn&rsquo;t just talking about LLMs. He was making a broader point about how these systems represent the world. Specifically, he was talking about a reliance on high-dimensional data and the need to move towards learning in lower-dimensional &ldquo;latent spaces.&rdquo;</p><h2 id=the-problem-with-high-dimensions>The problem with high dimensions<a hidden class=anchor aria-hidden=true href=#the-problem-with-high-dimensions>#</a></h2><p>Contemporary models typically process raw sensor data like pixels, point clouds, or discrete tokens of text. These representations are big, complex, and noisy; they contain a lot of irrelevant information that makes learning computationally expensive, inefficient, and brittle. Tiny changes in lighting, for example, can throw off an image processing model, even though this information isn&rsquo;t really related to what the object is in the real world.</p><p>If you think about it, this is exactly not how humans learn. <strong>We don&rsquo;t process every little detail in our environment.</strong> No, we pull out the important features and ignore the things that don&rsquo;t matter. Consider the example of catching a baseball. As it falls towards you, you&rsquo;re not paying attention to the color, texture, or even shape of the ball. You&rsquo;re certainly not thinking about the equations of motion that govern its flight. You&rsquo;re focused on a few key features — where it is relative to your glove (in 2D space) and how much bigger it&rsquo;s getting (in 1D space). From this, you make predictions about its path and adjust the position of your glove. The rest is noise.</p><p>While simple dimensionality reduction techniques (like, for example, Principal Component Analysis) reduce complexity, Yann’s point is different &mdash; the goal is not just compression, but the discovery of causal abstractions that explain why the world behaves the way it does.</p><h2 id=learning-latent-spaces>Learning latent spaces<a hidden class=anchor aria-hidden=true href=#learning-latent-spaces>#</a></h2><p>Incorporating latent spaces into learning is not a new idea. They are already foundational to how many generative models work. Some <a href=https://github.com/Stability-AI/generative-models>diffusion models</a>, for example, run certain processes in compressed latent spaces. <a href=https://arxiv.org/abs/1406.2661>Generative Adversarial Networks (GANs)</a> do something similar, mapping meaningful representations of data from compact latent spaces; these can then be manipulated to control the generation process (e.g., add a hat to a person&rsquo;s head).</p><p>Existing uses of latent spaces are largely static and domain-specific. For instance, a latent space trained to recognize cats does not help a model understand gravity or motion. Yann argues that truly intelligent systems need to learn latent representations of the physical world as well. These latent spaces <strong>capture the essential structure of the environment</strong> so the models can make accurate predictions when the little things change. He lays out a blueprint in <a href="https://openreview.net/forum?id=BZ5a1r-kVsf">&ldquo;A Path Towards Autonomous Machine Intelligence&rdquo;</a>.</p><h2 id=why-this-matters-for-the-future-of-ai>Why this matters for the future of AI<a hidden class=anchor aria-hidden=true href=#why-this-matters-for-the-future-of-ai>#</a></h2><p>Surveying the current landscape of AI, one might be forgiven for thinking the future is all about bigger models, more data, and faster hardware. If Yann is correct, the future of AI will not be written in pixels or tokens, but the invisible spaces beneath them — abstract structures that <strong>capture how the world works while looking nothing like it.</strong></p><p>We will see new architectures and methodologies that prioritize learning these latent spaces. Models will become more flexible and robust as they learn to focus on the underlying structure rather than surface-level details. They will need less data, burn less compute, and start to feel more like us.</p><h2 id=our-work>Our work<a hidden class=anchor aria-hidden=true href=#our-work>#</a></h2><p>We are exploring these concepts in our work on swarm robotics. In <a href=https://ieeexplore.ieee.org/document/9931405>&ldquo;Flocks, Mobs, and Figure Eights: Swarming as a Lemniscatic Arc&rdquo;</a>, we stabilized a swarm of agents in 3D space by controlling them indirectly through a 2D embedding space. This approach was motivated by a desire to use well-established linear control policies in reduced dimensions while preserving all of the functional properties of the swarm. We later generalized this approach for a broader set of curves in <a href=https://doi.org/10.1016/j.automatica.2025.112221>&ldquo;Emergent homeomorphic curves in swarms&rdquo;</a>, where we reconceptualized this embedding as <strong>a latent space that captures the essential structure of the swarm</strong>.</p><p>After <a href=https://github.com/tjards/UMAP_twisted_circles>exploring some of the properties</a> of the latent space using dimensionality reduction techniques like <a href=https://github.com/lmcinnes/umap>Uniform Manifold Approximation and Projection (UMAP)</a>, we are now working to apply reinforcement learning (RL) to control the orientation of the swarm by manipulating the latent space.</p><p>As a <strong>proof of concept</strong>, we applied Continuous Action Learning Automata (CALA) to learn the optimal orientation of the latent space and reported the initial findings <a href=https://github.com/tjards/hypospace_learning>here</a>. Collective learning was coordinated by a randomly selected leader agent. Below are two figures demonstrating, first, the evolution of the probability distribution functions for the two orientation parameters (altitude and azimuth) over time, and second, the combined rewards for each parameter over time.</p><p float=center><img src=/img/2025/hypospace_learning/RL_animation.gif width=60%><figcaption style=font-size:1em;margin-top:5px>Fig. 1 - Probability distribution function of altitude (blue) and azimuth (orange) parameters.</figcaption></p><p float=center><img src=/img/2025/hypospace_learning/RL_results.png width=60%><figcaption style=font-size:1em;margin-top:5px>Fig. 2 - Combined rewards for altitude (blue) and azimuth (orange) parameters over time.</figcaption></p><p>We have since incorporated this approach into our more general <a href=https://github.com/tjards/multi-agent_sim>Multi-Agent Simulator (m-a_s)</a> and are working on the following problems:</p><ol><li>Moving away from a leader-follower approach and towards more decentralized coordination of learning outcomes.</li><li>Investigating more complex tasks.</li><li>Exploring more sophisticated learning techniques.</li><li>Analyzing stability and convergence properties.</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/featured/>Featured</a></li><li><a href=http://localhost:1313/tags/ai/>Ai</a></li></ul></footer></article></main><footer class=site-footer><div class=footer-links><a href=/tags>tags</a>
<a href=/categories>categories</a></div><div class=container><p>Disclaimer: Views expressed here are my own and do not represent the opinions of my associates or my employer.</p><p>&copy; tjards 2025 |
Licenced under <a href=https://creativecommons.org/licenses/by/4.0/ target=_blank>Creative Commons</a>.</p><p>Built with <a href=https://gohugo.io/ target=_blank>Hugo</a>
using <a href=https://github.com/adityatelange/hugo-PaperMod target=_blank>PaperMod</a> theme.</p></footer></body></html>